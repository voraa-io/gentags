# Project Inventory: What We Have vs. What We Don't

**Date:** 2025-01-17  
**Status:** Week 1 Foundation Complete, Week 2 Not Started

---

## ‚úÖ WHAT WE HAVE (Week 1 Foundation)

### 1. Gentag Definition (FROZEN)
- **Location:** `src/gentags/config.py` (lines 11-32), `docs/STUDY1_LOCK.md`
- **Status:** ‚úÖ Complete and frozen
- **Definition:**
  - 1-4 words
  - Atomized (one idea)
  - Composable / scan-friendly
  - Embed-friendly
  - Emergent tag space (folksonomy-like, not taxonomy)
  - Zero-shot extraction (no examples / no ontology hints)
  - Explicit exclusions: ratings, predefined categories/slots, sentiment labels, summaries, ontology

### 2. Prompt Set (FROZEN)
- **Location:** `src/gentags/config.py` (lines 38-57), `docs/PROMPTS.md`
- **Status:** ‚úÖ Complete and frozen
- **Prompts:**
  - `minimal`
  - `anti_hallucination`
  - `short_phrase`
- **Version:** PROMPT_VERSION = "1.0"
- **Hash:** PROMPT_HASH generated from prompt dict
- **System Prompts:** Defined per provider (OpenAI/Grok use system prompts, Gemini/Claude don't)

### 3. Model Registry (FROZEN)
- **Location:** `src/gentags/config.py` (lines 75-117)
- **Status:** ‚úÖ Complete and frozen
- **Models:**
  - OpenAI: `gpt-5-nano`
  - Gemini: `gemini-2.5-flash`
  - Claude: `claude-sonnet-4-5`
  - Grok: `grok-4` (optional)
- **Model Params:** All set to `None` (provider defaults)
- **Pricing:** Defined for cost calculation

### 4. Reproducible Pipeline Module (v1.2)
- **Location:** `src/gentags/`
- **Status:** ‚úÖ Complete and working
- **Components:**
  - ‚úÖ `extractor.py`: `GentagExtractor` class with all provider clients
  - ‚úÖ `parsing.py`: `extract_json_list()` - robust JSON parsing
  - ‚úÖ `normalize.py`: `normalize_tag()`, `normalize_tag_eval()`, `filter_valid_tags()`
  - ‚úÖ `data.py`: `load_venue_data()` - loads CSV and extracts review texts (ratings excluded)
  - ‚úÖ `experiment.py`: `run_experiment()` - runs full experiment matrix
  - ‚úÖ `io.py`: `save_results()`, `save_raw_response()`, `load_results()`
  - ‚úÖ `schema.py`: `ExtractionResult`, `ExperimentConfig` dataclasses
  - ‚úÖ `config.py`: All frozen definitions, prompts, models
  - ‚úÖ `pipeline.py`: Comprehensive pipeline module (appears to be a consolidated version)
  - ‚úÖ `metrics.py`: (needs verification - may contain validation functions)

**Features:**
- Building prompts with reviews
- Calling each provider client (OpenAI, Gemini, Claude, Grok)
- Parsing model output into JSON list
- Filtering tags >4 words
- Storing metadata per run (run_id, exp_id, hashes, timing, token counts)
- Outputting results DataFrame with `tag_raw`, `tag_norm`, `tag_norm_eval`
- Sanity-check validator (`validate_tags()`)

### 5. Environment Setup
- **Location:** `pyproject.toml`, Poetry lock file
- **Status:** ‚úÖ Working
- **Features:**
  - Poetry environment works
  - Package imports functional
  - API keys can be loaded (via dotenv or system env vars)
  - Dependencies: pandas, openai, anthropic, google-genai, python-dotenv

### 6. Documentation
- **Status:** ‚úÖ Complete
- **Files:**
  - ‚úÖ `docs/STUDY1_LOCK.md` - Frozen methodological contract
  - ‚úÖ `docs/PROMPTS.md` - Prompt documentation
  - ‚úÖ `docs/MODEL_NOTES.md` - Model notes
  - ‚úÖ `docs/METHODS.md` - Methods documentation
  - ‚úÖ `docs/EVALUATION.md` - Evaluation documentation
  - ‚úÖ `README.md` - Project overview
  - ‚úÖ `SETUP.md` - Setup instructions

### 7. Scripts
- **Status:** ‚úÖ Complete
- **Files:**
  - ‚úÖ `scripts/sanity_check.py` - Week 1 smoke test script
  - ‚úÖ `scripts/run_phase1.py` - Full experiment runner
  - ‚úÖ `scripts/export_for_viewer.py` - Export utilities
  - ‚úÖ `scripts/export_tables.py` - Table export utilities

### 8. Data Loading
- **Status:** ‚úÖ Complete
- **Location:** `src/gentags/data.py`
- **Features:**
  - Loads venue CSV
  - Extracts review texts (ratings explicitly excluded)
  - Supports sampling
  - Returns DataFrame with `id`, `name`, `google_reviews` (as list)

---

## ‚ùå WHAT WE DON'T HAVE YET (Week 2+)

### 1. Results
- **Status:** ‚ùå No results yet
- **Missing:**
  - No extracted gentags dataset
  - No CSV files in `results/`
  - No stability numbers (Jaccard similarity metrics)
  - No pilot run stats (cost/time/error rate)
  - No actual extractions have been run

### 2. Week 1 "Paper Artifacts" (Partially Missing)
- **Status:** ‚ö†Ô∏è Partially complete
- **Have:**
  - ‚úÖ `docs/STUDY1_LOCK.md` (frozen contract)
  - ‚úÖ `scripts/sanity_check.py` (smoke test script)
- **Missing:**
  - ‚ùå Run manifest generator (`results/meta/manifest.json`)
  - ‚ùå Basic unit tests (`tests/` directory is empty)
    - No tests for `parse/normalize` functions
    - No tests for `extract_json_list()`
    - No tests for `normalize_tag()` / `normalize_tag_eval()`
    - No tests for `filter_valid_tags()`

### 3. Test Infrastructure
- **Status:** ‚ùå Empty
- **Location:** `tests/` directory exists but is empty
- **Missing:**
  - Unit tests for parsing
  - Unit tests for normalization
  - Unit tests for tag filtering
  - Integration tests for extraction

### 4. Manifest/Metadata Tracking
- **Status:** ‚ùå Not implemented
- **Location:** `results/meta/` directory exists but is empty
- **Missing:**
  - `manifest.json` generator
  - Run tracking metadata
  - Experiment summary files

---

## üìã NEXT STEPS (Week 2 Start)

### Immediate Priority: Smoke Test
Run a smoke test on **1 venue** (3-5 reviews):
- 3 prompts √ó 3 models √ó 1 run = 9 extractions
- **Goal:** Confirm you get:
  - ‚úÖ Valid JSON lists
  - ‚úÖ Tags <=4 words
  - ‚úÖ Metadata recorded
  - ‚úÖ File saved

**Command:**
```bash
python scripts/sanity_check.py
```

### After Smoke Test Passes: 10-Venue Pilot
Before running the full 500-venue experiment:
- Run 10 venues √ó 3 prompts √ó 3 models √ó 2 runs = 180 extractions
- **Goal:** Validate pipeline at scale, check costs, timing, error rates

**Command:**
```bash
python scripts/run_phase1.py --data data/sample/study1_venues_20250117.csv --sample-size 10
```

### Week 1 Completion Tasks (Optional but Recommended)
1. **Add unit tests:**
   - `tests/test_parsing.py` - Test `extract_json_list()`
   - `tests/test_normalize.py` - Test normalization functions
   - `tests/test_filter.py` - Test tag filtering

2. **Add manifest generator:**
   - `scripts/generate_manifest.py` - Generate `results/meta/manifest.json`
   - Track: run dates, venue counts, model/prompt combinations, file paths

---

## üìä Summary

| Category | Status | Completion |
|----------|--------|------------|
| **Foundation (Week 1)** | ‚úÖ Complete | 100% |
| **Code & Pipeline** | ‚úÖ Complete | 100% |
| **Documentation** | ‚úÖ Complete | 100% |
| **Scripts** | ‚úÖ Complete | 100% |
| **Results** | ‚ùå Not Started | 0% |
| **Tests** | ‚ùå Not Started | 0% |
| **Manifest** | ‚ùå Not Started | 0% |

**Overall Week 1 Status:** ‚úÖ **COMPLETE**  
**Week 2 Status:** ‚ùå **NOT STARTED** (no runs executed yet)

---

## üéØ Single Best Next Step

**Run the smoke test:**
```bash
python scripts/sanity_check.py
```

This will:
1. Load 1 venue from sample data
2. Run 1 extraction (1 model √ó 1 prompt √ó 1 run)
3. Validate output
4. Confirm pipeline is working

Once this passes, you're ready for Week 2 (pilot runs).

