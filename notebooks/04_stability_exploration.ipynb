{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Stability Analysis Exploration\n",
    "\n",
    "This notebook explores the stability analysis results and investigates issues.\n",
    "\n",
    "## Key Questions\n",
    "1. Why does Claude show 0.0 median cosine in summary but valid values in raw data?\n",
    "2. Is the \"lexically unstable but semantically stable\" claim validated?\n",
    "3. What's the token count vs variability correlation (S4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Paths\n",
    "TABLES_DIR = Path('../results/phase2/tables')\n",
    "DATA_DIR = Path('../data')\n",
    "PHASE1_DIR = Path('../results/phase1_downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "run_stability = pd.read_csv(TABLES_DIR / 'run_stability.csv')\n",
    "prompt_sensitivity = pd.read_csv(TABLES_DIR / 'prompt_sensitivity.csv')\n",
    "model_sensitivity = pd.read_csv(TABLES_DIR / 'model_sensitivity.csv')\n",
    "retention = pd.read_csv(TABLES_DIR / 'retention.csv')\n",
    "uncertainty = pd.read_csv(TABLES_DIR / 'uncertainty_dispersion.csv')\n",
    "\n",
    "print(f\"Run stability: {len(run_stability)} rows\")\n",
    "print(f\"Prompt sensitivity: {len(prompt_sensitivity)} rows\")\n",
    "print(f\"Model sensitivity: {len(model_sensitivity)} rows\")\n",
    "print(f\"Retention: {len(retention)} rows\")\n",
    "print(f\"Uncertainty: {len(uncertainty)} venues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Investigate Claude Issue\n",
    "\n",
    "The summary showed Claude with 0.0 median cosine, but raw data looks valid. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Claude data\n",
    "claude_data = run_stability[run_stability['model_key'] == 'claude']\n",
    "print(f\"Claude rows: {len(claude_data)}\")\n",
    "print(f\"\\nCosine similarity stats:\")\n",
    "print(claude_data['cosine_similarity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zeros\n",
    "zero_cosine = claude_data[claude_data['cosine_similarity'] == 0]\n",
    "print(f\"Claude rows with cosine = 0: {len(zero_cosine)} ({100*len(zero_cosine)/len(claude_data):.1f}%)\")\n",
    "\n",
    "# Check distribution\n",
    "print(f\"\\nCosine value distribution:\")\n",
    "print(claude_data['cosine_similarity'].value_counts(bins=10).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are zeros, look at what venues they are\n",
    "if len(zero_cosine) > 0:\n",
    "    print(\"Sample of zero-cosine venues:\")\n",
    "    print(zero_cosine[['venue_id', 'prompt_type', 'n_tags_run1', 'n_tags_run2', 'jaccard_norm_eval']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"Cosine similarity by model:\")\n",
    "print(run_stability.groupby('model_key')['cosine_similarity'].agg(['count', 'mean', 'median', 'std', 'min', 'max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Claim: Lexically Unstable but Semantically Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gap\n",
    "run_stability['semantic_gap'] = run_stability['cosine_similarity'] - run_stability['jaccard_norm_eval']\n",
    "\n",
    "print(\"Semantic Gap (Cosine - Jaccard):\")\n",
    "print(run_stability['semantic_gap'].describe())\n",
    "print(f\"\\n% with positive gap (cosine > jaccard): {100*(run_stability['semantic_gap'] > 0).mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Jaccard vs Cosine\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "ax1 = axes[0]\n",
    "for model in run_stability['model_key'].unique():\n",
    "    subset = run_stability[run_stability['model_key'] == model]\n",
    "    ax1.scatter(subset['jaccard_norm_eval'], subset['cosine_similarity'], \n",
    "                alpha=0.3, label=model, s=10)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='y=x')\n",
    "ax1.set_xlabel('Jaccard (Surface)')\n",
    "ax1.set_ylabel('Cosine (Semantic)')\n",
    "ax1.set_title('Surface vs Semantic Stability')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Gap distribution\n",
    "ax2 = axes[1]\n",
    "for model in run_stability['model_key'].unique():\n",
    "    subset = run_stability[run_stability['model_key'] == model]\n",
    "    ax2.hist(subset['semantic_gap'], bins=30, alpha=0.5, label=model)\n",
    "ax2.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Semantic Gap (Cosine - Jaccard)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Semantic Gap')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by model and prompt\n",
    "summary = run_stability.groupby(['model_key', 'prompt_type']).agg({\n",
    "    'cosine_similarity': ['median', 'mean'],\n",
    "    'jaccard_norm_eval': ['median', 'mean'],\n",
    "    'semantic_gap': ['median', 'mean'],\n",
    "    'mmc': ['median', 'mean'],\n",
    "}).round(3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S4: Sparsity Analysis (Token Count vs Variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load venue data to get reviews\n",
    "venues_df = pd.read_csv(DATA_DIR / 'study1_venues_20250117.csv')\n",
    "print(f\"Venues: {len(venues_df)}\")\n",
    "print(f\"Columns: {venues_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse google_reviews and compute token counts\n",
    "def count_tokens(reviews_str):\n",
    "    \"\"\"Count total words across all reviews for a venue.\"\"\"\n",
    "    try:\n",
    "        reviews = ast.literal_eval(reviews_str)\n",
    "        total_words = 0\n",
    "        for review in reviews:\n",
    "            if isinstance(review, dict) and 'text' in review:\n",
    "                total_words += len(review['text'].split())\n",
    "        return total_words\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def count_reviews(reviews_str):\n",
    "    \"\"\"Count number of reviews for a venue.\"\"\"\n",
    "    try:\n",
    "        reviews = ast.literal_eval(reviews_str)\n",
    "        return len(reviews)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "venues_df['total_tokens'] = venues_df['google_reviews'].apply(count_tokens)\n",
    "venues_df['n_reviews'] = venues_df['google_reviews'].apply(count_reviews)\n",
    "\n",
    "print(f\"Token count stats:\")\n",
    "print(venues_df['total_tokens'].describe())\n",
    "print(f\"\\nReview count stats:\")\n",
    "print(venues_df['n_reviews'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with uncertainty data\n",
    "sparsity_df = uncertainty.merge(\n",
    "    venues_df[['id', 'total_tokens', 'n_reviews']], \n",
    "    left_on='venue_id', \n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"Merged: {len(sparsity_df)} venues\")\n",
    "print(sparsity_df[['venue_id', 'total_tokens', 'n_reviews', 'mean_pairwise_distance']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation: tokens vs variability\n",
    "corr_tokens = sparsity_df['total_tokens'].corr(sparsity_df['mean_pairwise_distance'])\n",
    "corr_reviews = sparsity_df['n_reviews'].corr(sparsity_df['mean_pairwise_distance'])\n",
    "\n",
    "print(f\"Correlation (tokens vs variability): {corr_tokens:.3f}\")\n",
    "print(f\"Correlation (n_reviews vs variability): {corr_reviews:.3f}\")\n",
    "print(f\"\\nExpected: NEGATIVE (more evidence â†’ less variability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Tokens vs Variability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(sparsity_df['total_tokens'], sparsity_df['mean_pairwise_distance'], alpha=0.5)\n",
    "ax1.set_xlabel('Total Tokens (Evidence Amount)')\n",
    "ax1.set_ylabel('Mean Pairwise Distance (Variability)')\n",
    "ax1.set_title(f'S4: Sparsity vs Variability (r={corr_tokens:.3f})')\n",
    "\n",
    "ax2 = axes[1]\n",
    "# Bin by token count\n",
    "sparsity_df['token_bucket'] = pd.cut(sparsity_df['total_tokens'], \n",
    "                                      bins=[0, 100, 200, 300, 500, 1000, float('inf')],\n",
    "                                      labels=['<100', '100-200', '200-300', '300-500', '500-1000', '>1000'])\n",
    "sparsity_df.boxplot(column='mean_pairwise_distance', by='token_bucket', ax=ax2)\n",
    "ax2.set_xlabel('Token Bucket')\n",
    "ax2.set_ylabel('Mean Pairwise Distance (Variability)')\n",
    "ax2.set_title('Variability by Evidence Amount')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Stability Distribution by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of run stability\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Cosine similarity\n",
    "ax1 = axes[0]\n",
    "run_stability.boxplot(column='cosine_similarity', by='model_key', ax=ax1)\n",
    "ax1.set_title('Cosine Similarity (Semantic)')\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Cosine Similarity')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Jaccard\n",
    "ax2 = axes[1]\n",
    "run_stability.boxplot(column='jaccard_norm_eval', by='model_key', ax=ax2)\n",
    "ax2.set_title('Jaccard (Surface)')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('Jaccard')\n",
    "plt.suptitle('')\n",
    "\n",
    "# MMC\n",
    "ax3 = axes[2]\n",
    "run_stability.boxplot(column='mmc', by='model_key', ax=ax3)\n",
    "ax3.set_title('Mean Max Cosine (Paraphrase)')\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('MMC')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt sensitivity summary\n",
    "prompt_summary = prompt_sensitivity.groupby(['model_key', 'prompt1', 'prompt2']).agg({\n",
    "    'cosine_similarity': ['median', 'mean', 'std'],\n",
    "    'jaccard_norm_eval': ['median', 'mean'],\n",
    "}).round(3)\n",
    "print(prompt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of prompt sensitivity per model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for idx, model in enumerate(['claude', 'gemini', 'grok', 'openai']):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    model_data = prompt_sensitivity[prompt_sensitivity['model_key'] == model]\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = model_data.pivot_table(\n",
    "        values='cosine_similarity', \n",
    "        index='prompt1', \n",
    "        columns='prompt2', \n",
    "        aggfunc='median'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0.8, vmax=1.0, ax=ax)\n",
    "    ax.set_title(f'{model.upper()}: Prompt Sensitivity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model sensitivity summary\n",
    "model_summary = model_sensitivity.groupby(['prompt_type', 'model1', 'model2']).agg({\n",
    "    'cosine_similarity': ['median', 'mean', 'std'],\n",
    "    'jaccard_norm_eval': ['median', 'mean'],\n",
    "}).round(3)\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of model sensitivity per prompt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, prompt in enumerate(['minimal', 'anti_hallucination', 'short_phrase']):\n",
    "    ax = axes[idx]\n",
    "    prompt_data = model_sensitivity[model_sensitivity['prompt_type'] == prompt]\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = prompt_data.pivot_table(\n",
    "        values='cosine_similarity', \n",
    "        index='model1', \n",
    "        columns='model2', \n",
    "        aggfunc='median'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0.6, vmax=1.0, ax=ax)\n",
    "    ax.set_title(f'{prompt}: Model Sensitivity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention summary\n",
    "retention_summary = retention.groupby(['model_key', 'prompt_type']).agg({\n",
    "    'retention_cosine': ['median', 'mean'],\n",
    "    'retention_random': ['median', 'mean'],\n",
    "    'delta_retention': ['median', 'mean'],\n",
    "    'z_score_random': ['median', 'mean'],\n",
    "}).round(3)\n",
    "print(retention_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Retention vs Random Baseline\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(retention_summary))\n",
    "width = 0.35\n",
    "\n",
    "labels = [f\"{m}\\n{p}\" for (m, p) in retention_summary.index]\n",
    "actual = retention_summary[('retention_cosine', 'median')].values\n",
    "random = retention_summary[('retention_random', 'median')].values\n",
    "\n",
    "ax.bar(x - width/2, actual, width, label='Actual Retention', color='steelblue')\n",
    "ax.bar(x + width/2, random, width, label='Random Baseline', color='lightcoral')\n",
    "\n",
    "ax.set_ylabel('Cosine Similarity')\n",
    "ax.set_title('Retention: Gentags vs Random Baseline')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Table (Paper-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paper-ready summary\n",
    "paper_summary = run_stability.groupby('model_key').agg({\n",
    "    'cosine_similarity': lambda x: f\"{x.median():.3f} [{x.quantile(0.25):.3f}, {x.quantile(0.75):.3f}]\",\n",
    "    'jaccard_norm_eval': lambda x: f\"{x.median():.3f} [{x.quantile(0.25):.3f}, {x.quantile(0.75):.3f}]\",\n",
    "    'mmc': lambda x: f\"{x.median():.3f} [{x.quantile(0.25):.3f}, {x.quantile(0.75):.3f}]\",\n",
    "    'semantic_gap': lambda x: f\"{x.median():.3f}\",\n",
    "}).rename(columns={\n",
    "    'cosine_similarity': 'Cosine (median [IQR])',\n",
    "    'jaccard_norm_eval': 'Jaccard (median [IQR])',\n",
    "    'mmc': 'MMC (median [IQR])',\n",
    "    'semantic_gap': 'Gap'\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAPER-READY SUMMARY: Run Stability by Model\")\n",
    "print(\"=\"*80)\n",
    "print(paper_summary.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
